{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Walmart sales with Treeffuser\n",
    "\n",
    "In this tutorial we show how to use Treeffuser to model and forecast Walmart sales using the M5 forecasting dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we first install `treeffuser` and import the relevant libraries (if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:30:25.237555Z",
     "start_time": "2024-10-10T19:30:23.907554Z"
    }
   },
   "source": [
    "!pip install treeffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from treeffuser import Treeffuser\n",
    "\n",
    "# load autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a Kaggle account and download the data from https://www.kaggle.com/competitions/m5-forecasting-accuracy/data.\n",
    "\n",
    "If you're running this notebook in Colab, manually upload the necessary files (`calendar.csv`, `sales_train_validation.csv`, `sell_prices.csv`) to Colab by clicking the `Files` tab on the left sidebar and selecting `Upload`. Move the files into a new folder named `m5`. Once uploaded, the notebook will be able to read and process the data.\n",
    "\n",
    "If you're running this on your local machine, you can also use Kaggle's [command-line tool](https://www.kaggle.com/docs/api) and run the following from the command line:\n",
    "\n",
    "```bash\n",
    "cd ./m5 # path to folder where you want to save the data\n",
    "kaggle competitions download -c m5-forecasting-accuracy\n",
    "```\n",
    "\n",
    "Use your favorite tool to unzip the archive. In Linux/macOS,\n",
    "\n",
    "```bash\n",
    "unzip m5-forecasting-accuracy.zip\n",
    "```\n",
    "\n",
    "We'll be using the following files: `calendar.csv`, `sales_train_validation.csv`, and `sell_prices.csv`.\n",
    "\n",
    "\n",
    "<!-- - `calendar.csv` - Contains information about the dates on which the products are sold.\n",
    "- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store `[d_1 - d_1913]`.\n",
    "- `sell_prices.csv` - Contains information about the price of the products sold per store and date. -->\n",
    "<!-- - `sales_train_evaluation.csv`- Includes sales [`d_1 - d_1941]` (labels used for the Public leaderboard). -->\n",
    "<!-- - `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./m5\")  # change with path where you extracted the data archive\n",
    "\n",
    "calendar_df = pd.read_csv(data_path / \"calendar.csv\")\n",
    "sales_train_df = pd.read_csv(data_path / \"sales_train_validation.csv\")\n",
    "sell_prices_df = pd.read_csv(data_path / \"sell_prices.csv\")\n",
    "\n",
    "# add explicit columns for the day, month, year for ease of processing\n",
    "calendar_df[\"date\"] = pd.to_datetime(calendar_df[\"date\"])\n",
    "calendar_df[\"day\"] = calendar_df[\"date\"].dt.day\n",
    "calendar_df[\"month\"] = calendar_df[\"date\"].dt.month\n",
    "calendar_df[\"year\"] = calendar_df[\"date\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "### Preprocessing\n",
    "`sell_prices_df` contains the prices of each item in each store at a given time. The `wm_yr_wk` is a unique identifier for the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calendar_df` contains information about the dates on which the products were sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sales_train_df` contains the number of units sold for an item in each department and store. The sales are grouped by day: for example, the `d_1907` column has the number of units sold on the 1907-th day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align the sales data with the other DataFrames, we convert `sales_train_df` to a long format. We collapse the daily sales columns `d_{i}` into a single `sales` column, with an  additional `day` column indicating the day corresponding to each sales entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sales_data_from_wide_to_long(sales_df_wide):\n",
    "    index_vars = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    sales_df_long = pd.wide_to_long(\n",
    "        sales_df_wide.iloc[:100, 1:],\n",
    "        i=index_vars,\n",
    "        j=\"day\",\n",
    "        stubnames=[\"d\"],\n",
    "        sep=\"_\",\n",
    "    ).reset_index()\n",
    "\n",
    "    sales_df_long = sales_df_long.rename(columns={\"d\": \"sales\", \"day\": \"d\"})\n",
    "\n",
    "    sales_df_long[\"d\"] = \"d_\" + sales_df_long[\"d\"].astype(\n",
    "        \"str\"\n",
    "    )  # restore \"d_{i}\" format for day\n",
    "    return sales_df_long\n",
    "\n",
    "\n",
    "sales_train_df_long = convert_sales_data_from_wide_to_long(sales_train_df)\n",
    "sales_train_df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    sales_train_df_long[\"sales\"],\n",
    "    bins=np.arange(0, 10 + 1.5) - 0.5,\n",
    "    range=[0, 10],\n",
    "    density=True,\n",
    ")\n",
    "plt.xticks(range(10))\n",
    "plt.ylabel(\"Frequency of number of sales\")\n",
    "plt.title(\"Number of sales over the entire timespan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises sales data of 100 items over 1,913 days. For simplicity, we select the data from the first 365 days and discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_items = {len(sales_train_df_long['item_id'].unique())}\")\n",
    "print(f\"n_days = {len(sales_train_df_long['d'].unique())}\")\n",
    "\n",
    "sales_train_df_long[\"day_number\"] = sales_train_df_long[\"d\"].str.extract(\"(\\d+)\").astype(int)\n",
    "data = sales_train_df_long[sales_train_df_long[\"day_number\"] <= 365].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the lags of the previous 30 days and merge the sales, calendar, and price data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 30\n",
    "\n",
    "# sort data before computing lags\n",
    "data_index_vars = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "data.sort_values(data_index_vars + [\"day_number\"], inplace=True)\n",
    "\n",
    "for lag in range(1, n_lags + 1):\n",
    "    data[f\"sales_lag_{lag}\"] = data.groupby(by=data_index_vars)[\"sales\"].shift(lag)\n",
    "\n",
    "data = data.merge(calendar_df).merge(sell_prices_df)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treeffuser can handle **categorical columns**, but the dtype of those columns must be set to `category` in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"item_id\",\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"d\",\n",
    "    \"wm_yr_wk\",\n",
    "    \"weekday\",\n",
    "    \"event_name_1\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"snap_CA\",\n",
    "    \"snap_TX\",\n",
    "    \"snap_WI\",\n",
    "]\n",
    "data[categorical_columns] = data[categorical_columns].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each item, we take the first 300 days as train data and use the remaining 65 data as test data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = data[\"day_number\"] <= 300\n",
    "\n",
    "y_name = \"sales\"\n",
    "x_names = [\n",
    "    name for name in data.columns if name != y_name and name not in [\"day_number\", \"date\"]\n",
    "]\n",
    "\n",
    "X_train, y_train, dates_train = (\n",
    "    data[is_train][x_names],\n",
    "    data[is_train][y_name],\n",
    "    data[is_train][\"date\"],\n",
    ")\n",
    "X_test, y_test, dates_test = (\n",
    "    data[~is_train][x_names],\n",
    "    data[~is_train][y_name],\n",
    "    data[~is_train][\"date\"],\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic predictions with Treeffuser\n",
    "\n",
    "We regress the sales on the following covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(map(str, X_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Treeffuser(seed=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_samples = model.sample(X_test, n_samples=100, seed=0, n_steps=50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newsvendor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the practical relevance of accurate probabilistic predictions with an application to inventory management, using the newsvendor model \\citep{arrow1951optimal}. \n",
    "\n",
    "Assume that every day we decide how many units $q$ of an item to buy. \n",
    "We buy at a cost $c$ and sell at a price $p$. \n",
    "However, the demand $y$ is random, introducing uncertainty in our decision. \n",
    "The goal is to maximize the expected profit:\n",
    "$$\\max_{q} p~\\mathbb{E}\\left[\\min(q, y)\\right] - c q.$$\n",
    "The optimal solution to the newsvendor problem is to buy $q = F^{-1}\\left( \\frac{p-c}{p} \\right)$ units, where $F^{-1}$ is the quantile function of the distribution of $y$. \n",
    "\n",
    "Using Treeffuser, we can compute the quantiles from the samples and forecast the optimal quantity of units to buy.\n",
    "\n",
    "To compute profits, we use the observed prices, assume a margin of $50\\%$ over all products, and assume the actual number of sales of an item correspond to the demand of this item. We let Treeffuser, learn the conditional distribution of the demand of each item, estimate their quantiles, and thus determine the optimal quantity to buy. \n",
    "\n",
    "We use the held-out data to compute the profit made if Treeffuser was used to forecast the demand of each item and to manage the inventory of each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsvendor_utility(y_true, quantity_ordered, prices, stocking_cost):\n",
    "    \"\"\"\n",
    "    The newsvendor utility function with stock q, demand y, selling price p, stocking cost c is given by\n",
    "    $$ U(y, q, p, c) = p * min(y, q) - c * q $$\n",
    "    \"\"\"\n",
    "    utility = prices * np.minimum(y_true, quantity_ordered) - stocking_cost * quantity_ordered\n",
    "    return utility\n",
    "\n",
    "\n",
    "def newsvendor_optimal_quantity(y_samples, prices, stocking_cost):\n",
    "    \"\"\"\n",
    "    Returns the optimal quantity to order for the newsvendor problem.\n",
    "\n",
    "    It is given theoeretically by:\n",
    "        $$ q* = argmax_{q} E[U(y, q, p, c)] $$\n",
    "    which has a closed form solution,\n",
    "        $$ q* = F^{-1}( (p - c) / p) $$\n",
    "    where F is the CDF of the demand distribution\n",
    "    \"\"\"\n",
    "    # compute the target quantiles (p - c) / p\n",
    "    target_quantiles = (prices - stocking_cost) / prices\n",
    "    target_quantiles = np.maximum(target_quantiles, 0.0)\n",
    "\n",
    "    # compute the empirical quantities corresponding to the target quantiles\n",
    "    res = []\n",
    "    for i in range(y_samples.shape[1]):\n",
    "        optimal_quantities = np.quantile(y_samples[:, i], target_quantiles[i])\n",
    "        res.append(optimal_quantities)\n",
    "    optimal_quantities = np.array(res)\n",
    "    return optimal_quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't know the profit margin of each item, so we assume it is 50%.\n",
    "profit_margin = 0.5\n",
    "\n",
    "prices = X_test[\"sell_price\"].values\n",
    "stocking_cost = prices / (1 + profit_margin)\n",
    "\n",
    "# compute optimal quantities\n",
    "optimal_quantities = newsvendor_optimal_quantity(y_test_samples, prices, stocking_cost)\n",
    "\n",
    "# Treeffuser models continuous responses, hence we cast the predicted quantities into int\n",
    "optimal_quantities = optimal_quantities.astype(int)\n",
    "\n",
    "profit = newsvendor_utility(y_test, optimal_quantities, prices, stocking_cost)\n",
    "profit.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the cumulative profit, the average demand and inventory over time in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": dates_test,\n",
    "        \"profit\": profit,\n",
    "        \"demand\": y_test,\n",
    "        \"inventory\": optimal_quantities,\n",
    "        \"price\": prices,\n",
    "    }\n",
    ")\n",
    "\n",
    "# for each day, compute average demand and inventory weighted by price\n",
    "daily_summary = (\n",
    "    performance_data.groupby(\"date\", group_keys=False)\n",
    "    .apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "                \"profit\": x[\"profit\"].sum(),\n",
    "                \"avg_inventory_weighted\": (x[\"inventory\"] * x[\"price\"]).sum()\n",
    "                / x[\"price\"].sum(),  # price-weighted average inventory\n",
    "                \"avg_demand_weighted\": (x[\"demand\"] * x[\"price\"]).sum()\n",
    "                / x[\"price\"].sum(),  # price-weighted average demand\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(daily_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store color, alpha, linewidth, and linestyle for each line\n",
    "line_styles = {\n",
    "    \"cumulative_profit\": {\"color\": \"teal\", \"alpha\": 1, \"linewidth\": 2.5, \"linestyle\": \"-\"},\n",
    "    \"inventory\": {\n",
    "        \"color\": \"blue\",\n",
    "        \"alpha\": 0.5,\n",
    "        \"linewidth\": 1.5,\n",
    "        \"linestyle\": \"--\",\n",
    "    },\n",
    "    \"demand\": {\n",
    "        \"color\": \"orange\",\n",
    "        \"alpha\": 0.5,\n",
    "        \"linewidth\": 1.5,\n",
    "        \"linestyle\": \"--\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# create figure\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# define x-axis\n",
    "dates = pd.to_datetime(daily_summary[\"date\"])\n",
    "\n",
    "# plot cumulative profit\n",
    "ax1.plot(\n",
    "    dates,\n",
    "    daily_summary[\"profit\"].cumsum(),\n",
    "    **line_styles[\"cumulative_profit\"],\n",
    "    label=\"Cumulative Profit\",\n",
    ")\n",
    "ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "ax1.set_ylabel(\"Cumulative Profit ($)\", fontsize=12)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# create second y-axis for price-weighted inventory and demand\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    dates,\n",
    "    daily_summary[\"avg_inventory_weighted\"],\n",
    "    **line_styles[\"inventory\"],\n",
    "    label=\"Avg Inventory (Price Weighted)\",\n",
    ")\n",
    "ax2.plot(\n",
    "    dates,\n",
    "    daily_summary[\"avg_demand_weighted\"],\n",
    "    **line_styles[\"demand\"],\n",
    "    label=\"Avg Demand (Price Weighted)\",\n",
    ")\n",
    "ax2.set_ylabel(\"Avg Inventory and Demand (Units)\", fontsize=12)\n",
    "\n",
    "# combine all legends into one\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(\n",
    "    lines1 + lines2, labels1 + labels2, loc=\"upper center\", bbox_to_anchor=(0.5, 1.12), ncol=3\n",
    ")\n",
    "\n",
    "fig.autofmt_xdate()  # rotate x-tick labels\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
