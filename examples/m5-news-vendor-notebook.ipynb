{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Walmart sales with Treeffuser\n",
    "\n",
    "In this tutorial we show how to use Treeffuser to model and forecast Walmart sales using the M5 forecasting dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we first install `treeffuser` and import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install treeffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from treeffuser import Treeffuser\n",
    "\n",
    "# load autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a Kaggle account and download the data from https://www.kaggle.com/competitions/m5-forecasting-accuracy/data.\n",
    "\n",
    "If you're running this notebook in Colab, manually upload the necessary files (`calendar.csv`, `sales_train_validation.csv`, `sell_prices.csv`) to Colab by clicking the `Files` tab on the left sidebar and selecting `Upload`. Move the files into a new folder named `m5`. Once uploaded, the notebook will be able to read and process the data.\n",
    "\n",
    "If you're running this on your local machine, you can also use Kaggle's [command-line tool](https://www.kaggle.com/docs/api) and run the following from the command line:\n",
    "\n",
    "```bash\n",
    "cd ./m5 # path to folder where you want to save the data\n",
    "kaggle competitions download -c m5-forecasting-accuracy\n",
    "```\n",
    "\n",
    "Use your favorite tool to unzip the archive. In Linux/macOS,\n",
    "\n",
    "```bash\n",
    "unzip m5-forecasting-accuracy.zip\n",
    "```\n",
    "\n",
    "We'll be using the following files: `calendar.csv`, `sales_train_validation.csv`, and `sell_prices.csv`.\n",
    "\n",
    "\n",
    "<!-- - `calendar.csv` - Contains information about the dates on which the products are sold.\n",
    "- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store `[d_1 - d_1913]`.\n",
    "- `sell_prices.csv` - Contains information about the price of the products sold per store and date. -->\n",
    "<!-- - `sales_train_evaluation.csv`- Includes sales [`d_1 - d_1941]` (labels used for the Public leaderboard). -->\n",
    "<!-- - `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_path = \"./m5\"  # change with path where you extracted the data archive\n",
    "\n",
    "calendar_df = pd.read_csv(Path(data_path) / \"calendar.csv\")\n",
    "sales_train_df = pd.read_csv(Path(data_path) / \"sales_train_validation.csv\")\n",
    "sell_prices_df = pd.read_csv(Path(data_path) / \"sell_prices.csv\")\n",
    "\n",
    "# add explicit columns for the day, month, year for ease of processing\n",
    "calendar_df[\"date\"] = pd.to_datetime(calendar_df[\"date\"])\n",
    "calendar_df[\"day\"] = calendar_df[\"date\"].dt.day\n",
    "calendar_df[\"month\"] = calendar_df[\"date\"].dt.month\n",
    "calendar_df[\"year\"] = calendar_df[\"date\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "### Preprocessing\n",
    "`sell_prices_df` contains the prices of each item in each store at a given time. The `wm_yr_wk` is a unique identifier for the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calendar_df` contains information about the dates on which the products were sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sales_train_df` contains the number of units sold for an item in each department and store. The sales are grouped by day: for example, the `d_1907` column has the number of units sold on the 1907-th day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align the sales data with the other DataFrames, we convert `sales_train_df` to a long format. We collapse the daily sales columns `d_{i}` into a single `sales` column, with an  additional `day` column indicating the day corresponding to each sales entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sales_data_from_wide_to_long(sales_df_wide):\n",
    "    index_vars = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    sales_df_long = pd.wide_to_long(\n",
    "        sales_df_wide.iloc[:100, 1:],\n",
    "        i=index_vars,\n",
    "        j=\"day\",\n",
    "        stubnames=[\"d\"],\n",
    "        sep=\"_\",\n",
    "    ).reset_index()\n",
    "\n",
    "    sales_df_long = sales_df_long.rename(columns={\"d\": \"sales\", \"day\": \"d\"})\n",
    "\n",
    "    sales_df_long[\"d\"] = \"d_\" + sales_df_long[\"d\"].astype(\n",
    "        \"str\"\n",
    "    )  # restore \"d_{i}\" format for day\n",
    "    return sales_df_long\n",
    "\n",
    "\n",
    "sales_train_df_long = convert_sales_data_from_wide_to_long(sales_train_df)\n",
    "sales_train_df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    sales_train_df_long[\"sales\"],\n",
    "    bins=np.arange(0, 10 + 1.5) - 0.5,\n",
    "    range=[0, 10],\n",
    "    density=True,\n",
    ")\n",
    "plt.xticks(range(10))\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"number of sales over the entire timespan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises sales data of 100 items over 1,913 days. For simplicity, we select the data from the first 365 days and discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_items = {len(sales_train_df_long['item_id'].unique())}\")\n",
    "print(f\"n_days = {len(sales_train_df_long['d'].unique())}\")\n",
    "\n",
    "sales_train_df_long[\"day_number\"] = sales_train_df_long[\"d\"].str.extract(\"(\\d+)\").astype(int)\n",
    "data = sales_train_df_long[sales_train_df_long[\"day_number\"] <= 365].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the lags of the previous 30 days and merge the sales, calendar, and price data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 30\n",
    "\n",
    "# sort data before computing lags\n",
    "data_index_vars = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "data.sort_values(data_index_vars + [\"day_number\"], inplace=True)\n",
    "\n",
    "for lag in range(1, n_lags + 1):\n",
    "    data[f\"sales_lag_{lag}\"] = data.groupby(by=data_index_vars)[\"sales\"].shift(lag)\n",
    "\n",
    "data = data.merge(calendar_df).merge(sell_prices_df)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each item, we take the first 300 days as train data and use the remaining 65 data as test data for posterior predictive checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = data[\"day_number\"] <= 300\n",
    "data = data.drop(columns=[\"day_number\", \"day\"])\n",
    "\n",
    "y_name = \"sales\"\n",
    "x_names = [name for name in data.columns if name != y_name]\n",
    "\n",
    "X_train, y_train = data[is_train][x_names], data[is_train][y_name]\n",
    "X_test, y_test = data[~is_train][x_names], data[~is_train][y_name]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of the combined datasets are mostly categorical. We save the column indices in `cat_idx` as they will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_column_names = [\n",
    "    \"item_id\",\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"d\",\n",
    "    \"date\",\n",
    "    \"wm_yr_wk\",\n",
    "    \"weekday\",\n",
    "    \"wday\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "    \"event_name_1\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"snap_CA\",\n",
    "    \"snap_TX\",\n",
    "    \"snap_WI\",\n",
    "]\n",
    "cat_idx = [data.columns.get_loc(col) for col in cat_column_names]\n",
    "cat_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic predictions with Treeffuser\n",
    "\n",
    "We regress the sales on the following covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(map(str, X_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, Treeffuser supports only numpy.ndarray data with numerical values. Therefore, we convert the categorical columns into numerical labels and then convert the train and test data into numpy.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cat_column_names] = X_train[cat_column_names].apply(\n",
    "    lambda col: pd.Categorical(col).codes\n",
    ")\n",
    "\n",
    "X_train, y_train = X_train.to_numpy(), y_train.to_numpy()\n",
    "X_test, y_test = X_test.to_numpy(), y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit the data. We use `cat_idx` to tell Treeffuser which columns are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Treeffuser()\n",
    "model.fit(X_train, y_train, cat_idx=cat_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
